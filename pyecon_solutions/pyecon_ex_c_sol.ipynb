{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises C: Solutions\n",
    "_Version: October 31, 2020, see_ [PyEcon.org](https://pyecon.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Determinant of a matrix\n",
    "Create a function that recursively calculates the determinant of a matrix $M \\in \\mathcal{R}^{n\\times n}$. The recursive definition is given by\n",
    "\n",
    "$$\\det(M) = \\sum_{j = 1}^n (-1)^{1 + j} \\cdot M_{1, j} \\cdot \\det(M_{-1, -j}),$$\n",
    "\n",
    "where $M_{-1, -j}$ denotes the matrix $M$ without the first row and the $j$th column.\n",
    "\n",
    "1. Create the determinant function.\n",
    "2. Calculate the determinant of\n",
    "$\n",
    "M = \\begin{bmatrix}\n",
    "1 & 4 & 8 \\\\\n",
    "2 & 2 & 9\\\\\n",
    "3 & 7 & 1\n",
    "\\end{bmatrix}\n",
    "$.\n",
    "3. Compare your determinant function with `numpy.linalg.det()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as nplin\n",
    "\n",
    "\n",
    "def own_det(x):\n",
    "    \"\"\"Computes the determinant of M\"\"\"\n",
    "    n, m = x.shape\n",
    "    if n != m:\n",
    "        raise ValueError(\"M is not a square matrix\")\n",
    "    if n == 1:\n",
    "        return x\n",
    "    else:\n",
    "        det = 0\n",
    "        for j in range(n):\n",
    "            x_new = x.copy()\n",
    "            x_new = np.delete(x_new, 0, 0)\n",
    "            x_new = np.delete(x_new, j, 1)\n",
    "            det = det + x[0, j] * own_det(x_new) * (-1)**(1 + (j+1))\n",
    "        return det\n",
    "\n",
    "\n",
    "m = np.array([[1, 4, 8],\n",
    "              [2, 2, 9],\n",
    "              [3, 7, 1]])\n",
    "own_determinant = own_det(m)\n",
    "own_determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare determinants\n",
    "determinant = nplin.det(m)\n",
    "np.allclose(own_determinant, determinant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.rand(4, 4)\n",
    "np.allclose(own_det(r), nplin.det(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Inverse of a matrix\n",
    "Build a function that computes the inverse of matrix $M$ using the _adjoint method_, and your determinant function from Exercise 1. An element $C_{ij}$ of $C$, the cofactor matrix of $M$, can be written as\n",
    "\n",
    "$$C_{ij} = (-1)^{i + j} \\cdot \\det(M_{-i,-j}),$$\n",
    "\n",
    "and the inverse can be calculated by\n",
    "\n",
    "$$M^{-1} = \\frac{1}{\\det(M)} \\cdot C',$$\n",
    "\n",
    "where $C'$, the transposed matrix $C$, is the so-called adjugate matrix of $A$.\n",
    "\n",
    "1. Create the inverse function.\n",
    "2. Calculate the inverse of\n",
    "$\n",
    "M = \\begin{bmatrix}\n",
    "1 & 4 & 8 \\\\\n",
    "2 & 2 & 9\\\\\n",
    "3 & 7 & 1\n",
    "\\end{bmatrix}\n",
    "$.\n",
    "3. Compare your inverse function with `numpy.linalg.inv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as nplin\n",
    "\n",
    "\n",
    "def own_inv(x):\n",
    "    \"\"\"Computes the inverse of M\"\"\"\n",
    "    n, m = x.shape\n",
    "    if n != m:\n",
    "        raise ValueError(\"M is not a square matrix\")\n",
    "    c = np.empty([n, n])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            minor = x.copy()\n",
    "            minor = np.delete(minor, i, 0)\n",
    "            minor = np.delete(minor, j, 1)\n",
    "            c[i, j] = (-1)**((i+1) + (j+1)) * own_det(minor)\n",
    "    return (1 / own_det(x)) * c.T\n",
    "\n",
    "\n",
    "m = np.array([[1, 4, 8],\n",
    "              [2, 2, 9],\n",
    "              [3, 7, 1]])\n",
    "my_inverse = own_inv(m)\n",
    "my_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse = nplin.inv(m)\n",
    "np.allclose(my_inverse, inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.rand(4, 4)\n",
    "np.allclose(own_inv(r), nplin.inv(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Applying and observing the eigendecomposition\n",
    "\n",
    "Consider the following matrix:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "15.00 & 24.25 & 12.25 \\\\\n",
    "24.25 & 44.75 & 32.50 \\\\\n",
    "12.25 & 32.50 & 41.25\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The eigendecomposition of a symmetric matrix is $X = E\\Lambda E'$, where $E$ is the matrix of the corresponding eigenvectors and $\\Lambda$ is the diagonal matrix of eigenvalues. In the case of a symmetric matrix, $E$ is orthogonal, i.e., $EE' = I$, where $I$ is the identity matrix of appropriate dimensions.\n",
    "\n",
    "1. Determine the diagonal matrix of eigenvalues and the matrix of eigenvectors for $X$ with `numpy.linalg`.\n",
    "1. From your results of the eigendecomposition use the formula above and recompute $X$. Compare your results.\n",
    "1. Calculate $X^2$. The calculate $E \\Lambda^2 E'$. Compare your results. What do you observe?\n",
    "1. Compute the inverse by $X = E \\Lambda^{-1} E'$ and compare it to the inverse of $X$, e.g., by employing `numpy.linalg.inv()`.\n",
    "1. Compute $E_{(1)}' E_{(1)}$ and $E_{(1)} E_{(1)}'$, where $E_{(1)}$ represents the first column of matrix $E$.\n",
    "1. Calculate $\\sum_{i = 1}^n \\Lambda_{ii} E_{(i)} E_{(i)}'$, where $n$ refers to the number of columns of $X$ and $\\Lambda_{ii}$ is the $i$th diagonal element of $\\Lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as nplin\n",
    "\n",
    "# Eigendecomposition of matrix X\n",
    "x = np.array([[15, 24.25, 12.25],\n",
    "              [24.25, 44.75, 32.5],\n",
    "              [12.25, 32.5, 41.25]])\n",
    "lv, e = nplin.eig(x)\n",
    "lm = np.diag(lv)\n",
    "print('L = \\n', lm)\n",
    "print('E = \\n', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the eigendecomposition\n",
    "x_eig = e @ lm @ e.T\n",
    "# Alternative\n",
    "# x_eig = np.dot(np.dot(e, l), e.T)\n",
    "np.allclose(x, x_eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squaring a symmetric matrix by squaring its eigenvalue matrix\n",
    "x2 = x @ x\n",
    "x_eig_sq = e @ (lm @ lm) @ e.T\n",
    "np.allclose(x2, x_eig_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverting a symmetric matrix by inverting its eigenvalue matrix\n",
    "x_inv = e @ nplin.inv(lm) @ e.T\n",
    "np.allclose(x_inv, nplin.inv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two distinct vectors of the eigenvector matrix are orthogonal ...\n",
    "np.allclose(e[:, 0] @ e[:, 1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and the scalar product of a vector with itself is (in theory) 1\n",
    "e[:, 0] @ e[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without re-dimensioning the vectors, np.outer() computes the 'outer product'\n",
    "np.outer(e[:, 0], e[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way for reconstructing the original matrix by its eigendecomposition\n",
    "m = np.zeros(e.shape)\n",
    "for i in range(e.shape[1]):\n",
    "    m = m + lm[i, i] * np.outer(e[:, i], e[:, i])\n",
    "np.allclose(m, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Password generation and encryption\n",
    "1. Write a function that creates a password by randomly picking characters, i.e., letters, numbers, etc.\n",
    "1. Modify your function and include an option so that the function generates passwords where each character is unique.\n",
    "1. Encrypt and decrypt this text: \"MONTY PYTHON\"\n",
    "    1. Use the function of 2. to generate a 27 digits password.\n",
    "    1. Write an encoding function that you can use to encrypt uppercase letters texts and spaces.\n",
    "    1. Write a decryption function for decoding the previously encrypted text.\n",
    "\n",
    " _Hints:_\n",
    " * _You could use the modules random or numpy.random._\n",
    " * _You might benefit from studying the Python library on string methods._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def gen_password(digits=12):\n",
    "    signs = \"abcdefghijklmnopqrstuvwxyzäöü\"\n",
    "    signs += signs.upper()\n",
    "    signs += \"! '§$%&/()=?#+*~_<>|^°\"\n",
    "    signs += \"1234567890\"\n",
    "    return \"\".join(random.choices(signs, k=digits))\n",
    "\n",
    "\n",
    "gen_password(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_password(digits=12, unique=False):\n",
    "    signs = \"abcdefghijklmnopqrstuvwxyzäöü\"\n",
    "    signs += signs.upper()\n",
    "    signs += \"! '§$%&/()=?#+*~_<>|^°\"\n",
    "    signs += \"1234567890\"\n",
    "    if unique:\n",
    "        return \"\".join(random.sample(signs, k=digits))\n",
    "    else:\n",
    "        return \"\".join(random.choices(signs, k=digits))\n",
    "\n",
    "\n",
    "key = gen_password(27, unique=True)\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt_text(text, key, decrypt=False):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz \".upper()\n",
    "    if len(key) != len(alphabet):\n",
    "        raise ValueError(f\"'key' length must be {len(alphabet)}\")\n",
    "    if decrypt:\n",
    "        alphabet, key = (key, alphabet)\n",
    "    return text.translate(str.maketrans(alphabet, key))\n",
    "\n",
    "\n",
    "text = \"MONTY PYTHON\"\n",
    "encrypted = encrypt_text(text, key)\n",
    "encrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrypted = encrypt_text(encrypted, key, decrypt=True)\n",
    "decrypted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Simulation and estimation techniques\n",
    "Simulate data from a random variable $Y$, which can be thought to be the level of production of some country, with the following properties:\n",
    "\n",
    "$Y \\sim \\mathcal{\\ln N}(\\mu,\\,\\sigma^{2})$,\n",
    "\n",
    "with\n",
    "\n",
    "$E(Y) = AK^{1-\\alpha}L^\\alpha$,\n",
    "\n",
    "where\n",
    "\n",
    "$A = 1$ (Technology),\n",
    "\n",
    "$K  \\sim \\mathcal{U}(30, 50)$ (Capital),\n",
    "\n",
    "$L  \\sim \\mathcal{U}(20, 40)$ (Labour).\n",
    "\n",
    "Note that $\\mu = \\ln E(Y) - \\sigma^{2} / 2$. See the Log-normal distribution for details.\n",
    "\n",
    "#### I. Simulate data\n",
    "\n",
    "1. Set a seed so that your results are reproducible and define the parameters $\\sigma^{2} = 0.5$ and $\\alpha = 0.7$.\n",
    "1. Draw $n = 100000$ observations for $K$, $L$ and $Y$.\n",
    "1. Build a data frame of the observed vectors $K$, $L$ and $Y$.\n",
    "1. Add the logarithms of all columns to the dataset and name them $y$, $k$ and $l$.\n",
    "\n",
    "\n",
    "#### II. Ordinary least squares (OLS) estimation\n",
    "\n",
    "Validate your simulation by conducting an OLS estimation with the logarithmized data:\n",
    "\n",
    "Estimate the linear regression model\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = X\\boldsymbol{\\beta} + \\boldsymbol{e},\n",
    "$$\n",
    "\n",
    "where $X$ contains $\\boldsymbol{k}$ and $\\boldsymbol{l}$ as its columns as well as an intercept (a column of ones). $p$ is the number of columns of $X$.\n",
    "\n",
    "The OLS estimators are defined as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{b} = (X'X)^{-1}X'\\boldsymbol{y},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{\\hat{\\boldsymbol{e}}'\\hat{\\boldsymbol{e}}}{n - p},\n",
    "$$\n",
    "\n",
    "where $\\hat{\\boldsymbol{e}} = \\boldsymbol{y}-X\\boldsymbol{b}$.\n",
    "\n",
    "\n",
    "#### III Non-linear estimation algorithm\n",
    "\n",
    "Implement the Newton-Raphson algorithm based on $ln Y = \\boldsymbol{y}$, i.e., using the normally distributed data. The algorithm proceeds as follows:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta}_{j+1} = \\boldsymbol{\\theta}_j - H^{-1}\\boldsymbol{s},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\theta} = [\\boldsymbol{\\beta}', \\sigma^2]'$ and $H = \\frac{\\delta^2 \\ln L}{\\delta \\boldsymbol{\\theta} \\delta \\boldsymbol{\\theta}'}$ and $\\boldsymbol{s} = \\frac{\\delta \\ln L}{\\delta \\boldsymbol{\\theta}}$ are the _Hessian matrix_ and the gradient (here _score_), i.e., the matrix of the second derivatives and the vector of the first derivatives of the target function.\n",
    "\n",
    "In this case, the target is the logarithmized likelihood function\n",
    "\n",
    "$$\n",
    "\\ln L \\left(\\boldsymbol{\\theta}| \\boldsymbol{y}, X\\right) =  -\\frac{n}{2}\\ln \\left(2\\pi\\right) -\\frac{n}{2}\\ln \\left(\\sigma^2 \\right) - \\frac{\\left(\\boldsymbol{y}-X\\boldsymbol{\\beta} \\right)'\\left(\\boldsymbol{y}-X\\boldsymbol{\\beta} \\right)}{2\\sigma^2},\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "H =\n",
    "\\begin{bmatrix}\n",
    "-\\frac{1}{\\sigma^2}X'X & -\\frac{1}{\\sigma^4}X'\\boldsymbol{e} \\\\\n",
    "-\\frac{1}{\\sigma^4}\\boldsymbol{e}'X & \\frac{n}{2\\sigma^4} - \\frac{1}{\\sigma^6}\\boldsymbol{e}'\\boldsymbol{e}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\boldsymbol{s} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sigma^2}X'\\boldsymbol{y} - \\frac{1}{\\sigma^2}X'X\\boldsymbol{\\beta} \\\\\n",
    "- \\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}\\boldsymbol{e}'\\boldsymbol{e}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{e} = \\boldsymbol{y}-X\\boldsymbol{\\beta}$.\n",
    "\n",
    "* Start with some initial guess $\\boldsymbol{\\theta}_0 = (-0.3, 0.2, 0.8, 0.6)$.\n",
    "* Make sure the variance, i.e., the estimator for $\\theta_4 = \\sigma^2$, cannot turn negative in any iteration step.\n",
    "* Stop the algorithm if $\\text{abs}(\\boldsymbol{\\theta}_{j+1} - \\boldsymbol{\\theta}_j) < 0.00001 \\cdot  \\boldsymbol{1}$.\n",
    "* Compare your estimates obtained from the Newton-Raphson algorithm to the results of the analytical solution $\\boldsymbol{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as nplin\n",
    "import pandas as pd\n",
    "\n",
    "# Setting random seed\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Setting parameters\n",
    "n = 100000\n",
    "sigma2 = 0.5\n",
    "alpha = 0.7\n",
    "\n",
    "# Simulating data\n",
    "a = 1\n",
    "kv = 30 + np.random.rand(n) * (50 - 30)\n",
    "lv = 20 + np.random.rand(n) * (40 - 20)\n",
    "mu = np.log(a * kv**(1-alpha) * lv**alpha) - sigma2/2\n",
    "y = np.random.lognormal(mu, np.sqrt(sigma2), n)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the DataFrame\n",
    "data = pd.DataFrame({\"Y\": y, \"K\": kv, \"L\": lv})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending the DataFrame\n",
    "data[\"k\"] = np.log(data[\"K\"])\n",
    "data[\"l\"] = np.log(data[\"L\"])\n",
    "data[\"y\"] = np.log(data[\"Y\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS estimation\n",
    "y = data[\"y\"].values\n",
    "x = np.array([np.ones(n), data[\"k\"], data[\"l\"]]).T\n",
    "n, p = x.shape\n",
    "b = nplin.inv(x.T @ x) @ (x.T @ y)\n",
    "e = y - x @ b\n",
    "sigma2_est = (e @ e) / (n - p)\n",
    "print(\"b = \\n\", b)\n",
    "print(\"ln(A) = \\n\", np.log(a))\n",
    "print(\"\\nNote that b[0] is not an estimator for ln{A}\"\n",
    "      \" but for ln{A} - sigma2/2.\")\n",
    "print(\"ln(A) - sigma^2/2 = \\n\", np.log(a) - sigma2/2, \"\\n\")\n",
    "print(\"sigma2_est = \\n\", sigma2_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton-Raphson non-linear optimization\n",
    "\n",
    "\n",
    "def loglike(theta, y, x):\n",
    "    n = y.shape[0]\n",
    "    e = y - x @ theta[:-1]\n",
    "    llv = -(n/2)*np.log(2 * np.pi) - (n/2)*np.log(theta[-1]) \\\n",
    "          - (e.T @ e) / (2 * theta[-1])\n",
    "    return llv\n",
    "\n",
    "\n",
    "def score(theta, y, x):\n",
    "    n = y.shape[0]\n",
    "    e = y - x @ theta[:-1]\n",
    "    s1 = (x.T @ y - x.T @ x @ theta[:-1]) / theta[-1]\n",
    "    s2 = np.atleast_1d(-n / (2 * theta[-1]) + (e.T @ e) / (2 * theta[-1]**2))\n",
    "    return np.concatenate([s1, s2])\n",
    "\n",
    "\n",
    "def hessian(theta, y, x):\n",
    "    n = y.shape[0]\n",
    "    e = y - x @ theta[:-1]\n",
    "    h11 = -(x.T @ x) / theta[-1]\n",
    "    h12 = np.atleast_2d(-(x.T @ e) / theta[-1]**2).T\n",
    "    h22 = n / (2 * theta[-1]**2) - (e.T @ e) / theta[-1]**3\n",
    "    return np.block([[h11, h12],\n",
    "                     [h12.T, h22]])\n",
    "\n",
    "\n",
    "def maximize_loglike(theta0, y, x, eps=1e-5):\n",
    "    theta = theta0\n",
    "    while True:\n",
    "        pre = theta\n",
    "        theta = theta - nplin.inv(hessian(theta, y, x)) @ score(theta, y, x)\n",
    "        if theta[-1] <= 0:\n",
    "            theta[-1] = 1e-8\n",
    "        if np.all(np.abs(theta - pre) < eps):\n",
    "            break\n",
    "        # optionally printing process\n",
    "        print(theta)\n",
    "    return theta, loglike(theta, y, x)\n",
    "\n",
    "\n",
    "# maximize multivariate normal log-likelihood\n",
    "theta0 = np.array([-0.3, 0.2, 0.8, 0.6])\n",
    "theta_max, llv_max = maximize_loglike(theta0, y, x)\n",
    "print(\"loglike_ml: \\n\", llv_max)\n",
    "print(\"b_ml = \\n\", theta_max[:-1])\n",
    "print(\"sigma2_ml = \\n\", theta_max[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with OLS estimate\n",
    "np.allclose(np.block([b, sigma2_est]), theta_max, atol=1e-4, rtol=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
